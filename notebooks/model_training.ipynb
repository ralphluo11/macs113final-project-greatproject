{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"1\",\n",
    "        \"spark.executor.cores\": \"4\",\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\": \"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==1.0.5)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==1.0.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.5 python-dateutil-2.9.0.post0\n",
      "\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/82/c1fe128f3526b128cfd185580ba40d01371c5d299fcf7f77968e22dfcc2e/scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from scipy==1.4.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.4.1\n",
      "\n",
      "Collecting matplotlib==3.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/c2/71fcf957710f3ba1f09088b35776a799ba7dd95f7c2b195ec800933b276b/matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4MB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/0c/0e3c05b1c87bb6a1c76d281b0f35e78d2d80ac91b5f8f524cebf77f51049/pyparsing-3.1.4-py3-none-any.whl (104kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/tmp/1748371303476-0/lib/python3.7/site-packages (from matplotlib==3.2.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib==3.2.1)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/f9/695d6bedebd747e5eb0fe8fad57b72fdf25411273a39791cde838d5a8f51/cycler-0.11.0-py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/77/e3046bf19720b22e3e0b7c12e28f6f2c0c18a213fb91a56cea640862270f/kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.2.1)\n",
      "Collecting typing-extensions; python_version < \"3.8\" (from kiwisolver>=1.0.1->matplotlib==3.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl\n",
      "Installing collected packages: pyparsing, cycler, typing-extensions, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.4.5 matplotlib-3.2.1 pyparsing-3.1.4 typing-extensions-4.7.1"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==1.0.5\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"scipy==1.4.1\", \"https://pypi.org/simple\")\n",
    "sc.install_pypi_package(\"matplotlib==3.2.1\", \"https://pypi.org/simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Repartition and persist in memory\n",
    "# sdf = spark.read.option(\"header\", True)\\\n",
    "#                 .option(\"inferSchema\", True)\\\n",
    "#                 .option(\"quote\", \"\\\"\")\\\n",
    "#                 .option(\"escape\", \"\\\"\")\\\n",
    "#                 .option(\"multiLine\", True)\\\n",
    "#                 .csv(\"s3://truegraph/data_random_split.csv\")\n",
    "# sdf = sdf.repartition(16)\n",
    "\n",
    "# # Save as Parquet to S3 \n",
    "# sdf.write.mode(\"overwrite\").parquet(\"s3://truegraph/data/parquet/data_random_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet back into Spark \n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\").persist()\n",
    "\n",
    "# Repartition again for better parallelism\n",
    "sdf = sdf.repartition(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts ‚Üí Train: 27978, Val: 6996, Test: 1300\n",
      "Validation ‚Üí Accuracy: 0.5945,  F1: 0.5883\n",
      "Test ‚Üí Accuracy: 0.6146,  F1: 0.6119\n",
      "‚úÖ Model saved to s3://truegraph/models/bias_classifier_pca"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover,\n",
    "    HashingTF, IDF, PCA\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Rename and select necessary columns for the pipeline \n",
    "data = sdf.select(\n",
    "    col(\"ID\"),\n",
    "    col(\"bias\").cast(\"int\").alias(\"label\"),  # ensure numeric\n",
    "    col(\"content\").alias(\"text\"),\n",
    "    col(\"split\")  # \"train\", \"valid\", or \"test\"\n",
    ")\n",
    "\n",
    "# Filter by split the dataset \n",
    "train_df = data.filter(col(\"split\") == \"train\")\n",
    "val_df   = data.filter(col(\"split\") == \"valid\")\n",
    "test_df  = data.filter(col(\"split\") == \"test\")\n",
    "\n",
    "print(f\"Counts ‚Üí Train: {train_df.count()}, Val: {val_df.count()}, Test: {test_df.count()}\")\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"label\", \"text\"]).persist()\n",
    "val_df   = val_df.dropna(subset=[\"label\", \"text\"]).persist()\n",
    "test_df  = test_df.dropna(subset=[\"label\", \"text\"]).persist()\n",
    "\n",
    "# Define Spark ML pipeline with PCA and Logistic Regression \n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "remover   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf       = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pca       = PCA(k=500, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "lr        = LogisticRegression(featuresCol=\"pcaFeatures\", labelCol=\"label\", maxIter=20, regParam=0.1)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, pca, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate on val/test\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "for df, name in [(val_df, \"Validation\"), (test_df, \"Test\")]:\n",
    "    preds = model.transform(df)\n",
    "    acc = evaluator_acc.evaluate(preds)\n",
    "    f1  = evaluator_f1.evaluate(preds)\n",
    "    print(f\"{name} ‚Üí Accuracy: {acc:.4f},  F1: {f1:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.write().overwrite().save(\"s3://truegraph/models/bias_classifier_pca\")\n",
    "print(\"‚úÖ Model saved to s3://truegraph/models/bias_classifier_pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts ‚Üí Train: 27978 Val: 6996 Test: 1300\n",
      "Validation ‚Üí Accuracy: 0.5433, F1: 0.5385\n",
      "Test ‚Üí Accuracy: 0.5662, F1: 0.5627\n",
      "‚úÖ Best model saved to s3://truegraph/models/bias_classifier_pca_tuned"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, HashingTF, IDF, PCA\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load data from Parquet to ensure efficient processing \n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\").repartition(128)\n",
    "\n",
    "# Select needed columns and rename them for modeling\n",
    "clean_df = sdf.select(\n",
    "    col(\"bias\").alias(\"label\"),\n",
    "    col(\"content\").alias(\"text\"),\n",
    "    col(\"split\"),\n",
    "    col(\"ID\")\n",
    ").filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "# Split into train/val/test sets and persist them \n",
    "train_df = clean_df.filter(col(\"split\") == \"train\").persist()\n",
    "val_df   = clean_df.filter(col(\"split\") == \"valid\").persist()\n",
    "test_df  = clean_df.filter(col(\"split\") == \"test\").persist()\n",
    "\n",
    "print(\"Counts ‚Üí\", \"Train:\", train_df.count(), \"Val:\", val_df.count(), \"Test:\", test_df.count())\n",
    "\n",
    "# Define Spark ML pipeline\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "remover   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf       = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pca       = PCA(k=100, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "lr        = LogisticRegression(featuresCol=\"pcaFeatures\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, pca, lr])\n",
    "\n",
    "# Define hyperparameter grid to search \n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
    "    .addGrid(lr.maxIter, [10, 20, 30, 50])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Define evaluator to view model performance \n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# Cross-validation setup to find best hyperparameters \n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ").setSeed(SEED)\n",
    "\n",
    "# Fit model with grid search\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "for df, name in [(val_df, \"Validation\"), (test_df, \"Test\")]:\n",
    "    preds = cv_model.transform(df)\n",
    "    acc = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    ).evaluate(preds)\n",
    "    f1 = evaluator.evaluate(preds)\n",
    "    print(f\"{name} ‚Üí Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "cv_model.bestModel.write().overwrite().save(\"s3://truegraph/models/bias_classifier_hyper_tuned\")\n",
    "print(\"‚úÖ Best model saved to s3://truegraph/models/bias_classifier_hyper_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts ‚Üí 27978 6996 1300\n",
      "üîç Best PCA k = 1500\n",
      "Validation ‚Üí Accuracy: 0.6139,  F1: 0.6110\n",
      "Test ‚Üí Accuracy: 0.6138,  F1: 0.6131\n",
      "‚úÖ Model saved to s3://truegraph/models/bias_classifier_pca_tuned"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, HashingTF, IDF, PCA\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load the Parquet dataset \n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\") \\\n",
    "            .select(\n",
    "              col(\"bias\").alias(\"label\"),\n",
    "              col(\"content\").alias(\"text\"),\n",
    "              col(\"split\")\n",
    "            ) \\\n",
    "            .filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "train_df = sdf.filter(col(\"split\") == \"train\").persist()\n",
    "val_df   = sdf.filter(col(\"split\") == \"valid\").persist()\n",
    "test_df  = sdf.filter(col(\"split\") == \"test\").persist()\n",
    "\n",
    "print(\"Counts ‚Üí\", train_df.count(), val_df.count(), test_df.count())\n",
    "\n",
    "# Build pipeline stages and set up PCA with Logistic Regression\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "remover   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf       = IDF(inputCol=\"rawFeatures\",   outputCol=\"features\")\n",
    "pca       = PCA(inputCol=\"features\",      outputCol=\"pcaFeatures\")\n",
    "# fix LR hyperparameters to reasonable defaults\n",
    "lr        = LogisticRegression(featuresCol=\"pcaFeatures\", labelCol=\"label\",\n",
    "                               regParam=0.1, maxIter=20)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, pca, lr])\n",
    "\n",
    "# Set up a grid that only tunes PCA.k to find the best number of components\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(pca.k, [200, 300, 500, 1000, 1500])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Evaluator (F1) \n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ").setSeed(SEED)\n",
    "\n",
    "# Fit on the train split (only PCA is being tuned)\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Inspect the best PCA k\n",
    "bestPipelineModel = cv_model.bestModel\n",
    "bestPCAmodel      = bestPipelineModel.stages[4]   # PCA is the 5th stage\n",
    "print(\"üîç Best PCA k =\", bestPCAmodel.getK())\n",
    "\n",
    "# Evaluate on validation and test\n",
    "for df, name in [(val_df, \"Validation\"), (test_df, \"Test\")]:\n",
    "    preds = bestPipelineModel.transform(df)\n",
    "    acc   = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "            ).evaluate(preds)\n",
    "    f1    = evaluator.evaluate(preds)\n",
    "    print(f\"{name} ‚Üí Accuracy: {acc:.4f},  F1: {f1:.4f}\")\n",
    "\n",
    "# Save the tuned model\n",
    "bestPipelineModel.write().overwrite().save(\"s3://truegraph/models/bias_classifier_pca_tuned\")\n",
    "print(\"‚úÖ Model saved to s3://truegraph/models/bias_classifier_pca_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 27978, Test rows: 1300\n",
      "Random Forest ‚Üí Test Accuracy: 0.5377, F1: 0.5053\n",
      "‚úÖ Random Forest model saved to s3://truegraph/models/bias_rf_pca100_n100_d10"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover,\n",
    "    HashingTF, IDF, PCA\n",
    ")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load and prepare data from Parquet \n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\") \\\n",
    "    .select(\n",
    "      col(\"bias\").alias(\"label\"),\n",
    "      col(\"content\").alias(\"text\"),\n",
    "      col(\"split\")\n",
    "    ) \\\n",
    "    .filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "train_df = sdf.filter(col(\"split\") == \"train\").persist()\n",
    "test_df  = sdf.filter(col(\"split\") == \"test\").persist()\n",
    "\n",
    "print(f\"Train rows: {train_df.count()}, Test rows: {test_df.count()}\")\n",
    "\n",
    "# Build pipeline stages\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "remover   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf       = IDF(inputCol=\"rawFeatures\",   outputCol=\"features\")\n",
    "pca       = PCA(inputCol=\"features\",      outputCol=\"pcaFeatures\", k=100)\n",
    "\n",
    "# Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"pcaFeatures\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,     # number of trees in the forest\n",
    "    maxDepth=10,      # maximum depth of each tree\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, pca, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Evaluate on test set\n",
    "preds = model.transform(test_df)\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "acc = evaluator_acc.evaluate(preds)\n",
    "f1  = evaluator_f1.evaluate(preds)\n",
    "print(f\"Random Forest ‚Üí Test Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Save the RF model\n",
    "model.write().overwrite().save(\"s3://truegraph/models/bias_rf_pca100_n100_d10\")\n",
    "print(\"‚úÖ Random Forest model saved to s3://truegraph/models/bias_rf_pca100_n100_d10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts ‚Üí Train: 27978, Valid: 6996, Test: 1300\n",
      "üîç Best numTrees:            500\n",
      "üîç Best maxDepth:            20\n",
      "üîç Best minInstancesPerNode: 5\n",
      "Validation ‚Üí Accuracy: 0.5640,  F1: 0.5527\n",
      "Test ‚Üí Accuracy: 0.5592,  F1: 0.5411\n",
      "‚úÖ Random Forest (tuned) saved to s3://truegraph/models/bias_rf_pca100_tuned"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF, PCA\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load and prepare data from Parquet\n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\") \\\n",
    "    .select(\n",
    "        col(\"bias\").alias(\"label\"),\n",
    "        col(\"content\").alias(\"text\"),\n",
    "        col(\"split\")\n",
    "    ) \\\n",
    "    .filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "train_df = sdf.filter(col(\"split\") == \"train\").persist()\n",
    "val_df   = sdf.filter(col(\"split\") == \"valid\").persist()\n",
    "test_df  = sdf.filter(col(\"split\") == \"test\").persist()\n",
    "\n",
    "print(f\"Counts ‚Üí Train: {train_df.count()}, Valid: {val_df.count()}, Test: {test_df.count()}\")\n",
    "\n",
    "# Build pipeline stages\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
    "remover   = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf       = IDF(inputCol=\"rawFeatures\",   outputCol=\"features\")\n",
    "pca       = PCA(inputCol=\"features\",      outputCol=\"pcaFeatures\", k=100)\n",
    "rf        = RandomForestClassifier(\n",
    "    featuresCol=\"pcaFeatures\",\n",
    "    labelCol=\"label\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, pca, rf])\n",
    "\n",
    "# Set up hyperparameter grid for Random Forest \n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [100, 200, 500])\n",
    "    .addGrid(rf.maxDepth, [10, 20, 30])\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5, 10])\n",
    "    .addGrid(rf.featureSubsetStrategy, [\"auto\",\"sqrt\",\"log2\"]) \n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Evaluator (F1)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=2\n",
    ").setSeed(SEED)\n",
    "\n",
    "# Fit on the train split\n",
    "cvModel = cv.fit(train_df)\n",
    "\n",
    "# Inspect best RF parameters\n",
    "bestModel = cvModel.bestModel\n",
    "bestRF    = bestModel.stages[-1]  # RandomForest is last stage\n",
    "\n",
    "print(\"üîç Best numTrees:           \", bestRF.getNumTrees)\n",
    "print(\"üîç Best maxDepth:           \", bestRF.getMaxDepth())\n",
    "print(\"üîç Best minInstancesPerNode:\", bestRF.getMinInstancesPerNode())\n",
    "\n",
    "# Evaluate on validation and test\n",
    "for df, name in [(val_df, \"Validation\"), (test_df, \"Test\")]:\n",
    "    preds = bestModel.transform(df)\n",
    "    acc   = MulticlassClassificationEvaluator(\n",
    "                labelCol=\"label\",\n",
    "                predictionCol=\"prediction\",\n",
    "                metricName=\"accuracy\"\n",
    "            ).evaluate(preds)\n",
    "    f1    = evaluator.evaluate(preds)\n",
    "    print(f\"{name} ‚Üí Accuracy: {acc:.4f},  F1: {f1:.4f}\")\n",
    "\n",
    "# Save the tuned model that achieved the best performance\n",
    "bestModel.write().overwrite().save(\"s3://truegraph/models/bias_rf_pca100_tuned\")\n",
    "print(\"‚úÖ Random Forest (tuned) saved to s3://truegraph/models/bias_rf_pca100_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble ‚Üí Test Accuracy: 0.6377,  F1: 0.6313"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load my tuned models\n",
    "lr_model = PipelineModel.load(\"s3://truegraph/models/bias_classifier_pca_tuned\")\n",
    "rf_model = PipelineModel.load(\"s3://truegraph/models/bias_rf_pca100_tuned\")\n",
    "\n",
    "# Load test data (with ID) from Parquet\n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\") \\\n",
    "    .select(\"ID\", col(\"bias\").alias(\"label\"), col(\"content\").alias(\"text\"), \"split\") \\\n",
    "    .filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "test_df = sdf.filter(col(\"split\") == \"test\").cache()\n",
    "\n",
    "# Get probability vectors from each model\n",
    "lr_preds = lr_model.transform(test_df) \\\n",
    "    .select(\"ID\", \"label\", \"probability\") \\\n",
    "    .witColumnRenamed(\"probability\", \"prob_lr\")\n",
    "\n",
    "rf_preds = rf_model.transform(test_df) \\\n",
    "    .select(\"ID\", \"probability\") \\\n",
    "    .withColumnRenamed(\"probability\", \"prob_rf\")\n",
    "\n",
    "# Join the two sets of probabilities on ID\n",
    "ensemble = lr_preds.join(rf_preds, on=\"ID\")\n",
    "\n",
    "# Define UDFs for averaging and argmax (returning DoubleType)\n",
    "avg_udf = udf(\n",
    "    lambda v1, v2: Vectors.dense([(x + y) / 2 for x, y in zip(v1, v2)]),\n",
    "    VectorUDT()\n",
    ")\n",
    "argmax_udf = udf(\n",
    "    lambda v: float(max(range(len(v)), key=lambda i: v[i])),\n",
    "    DoubleType()\n",
    ")\n",
    "\n",
    "# Compute the ensemble probabilities and final prediction\n",
    "ensemble = (ensemble\n",
    "    .withColumn(\"avg_prob\", avg_udf(\"prob_lr\", \"prob_rf\"))\n",
    "    .withColumn(\"prediction\", argmax_udf(\"avg_prob\"))\n",
    ")\n",
    "\n",
    "# Ensure both label and prediction are DoubleType\n",
    "ensemble = (ensemble\n",
    "    .withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "    .withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# Evaluate ensemble performance\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "acc = evaluator_acc.evaluate(ensemble)\n",
    "f1  = evaluator_f1.evaluate(ensemble)\n",
    "print(f\"Ensemble ‚Üí Test Accuracy: {acc:.4f},  F1: {f1:.4f}\")\n",
    "\n",
    "# Save ensemble predictions\n",
    "ensemble.select(\"ID\", \"label\", \"prediction\", \"avg_prob\") \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .parquet(\"s3://truegraph/models/ensemble_preds_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Confusion matrix (label √ó prediction):\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|  206|\n",
      "|  0.0|       1.0|   38|\n",
      "|  0.0|       2.0|  158|\n",
      "|  1.0|       0.0|   32|\n",
      "|  1.0|       1.0|  158|\n",
      "|  1.0|       2.0|  109|\n",
      "|  2.0|       0.0|   89|\n",
      "|  2.0|       1.0|   45|\n",
      "|  2.0|       2.0|  465|\n",
      "+-----+----------+-----+\n",
      "\n",
      "üìä Class‚Äêwise metrics:\n",
      " Class 0 ‚Üí Precision: 0.6300,  Recall: 0.5124,  F1: 0.5652\n",
      " Class 1 ‚Üí Precision: 0.6556,  Recall: 0.5284,  F1: 0.5852\n",
      " Class 2 ‚Üí Precision: 0.6352,  Recall: 0.7763,  F1: 0.6987\n",
      "\n",
      "Overall Accuracy = 0.6377\n",
      "Weighted  F1      = 0.6313"
     ]
    }
   ],
   "source": [
    "# Imports the package for evaluation \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Load the ensemble predictions previously saved\n",
    "ensemble = spark.read.parquet(\"s3://truegraph/models/ensemble_preds_test\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion matrix (label √ó prediction):\")\n",
    "ensemble.groupBy(\"label\", \"prediction\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"label\", \"prediction\") \\\n",
    "        .show()\n",
    "\n",
    "# Build an RDD of (prediction, label) and compute metrics\n",
    "pred_label_rdd = ensemble.select(\"prediction\", \"label\") \\\n",
    "                          .rdd.map(lambda r: (r[0], r[1]))\n",
    "metrics = MulticlassMetrics(pred_label_rdd)\n",
    "\n",
    "# Get the distinct labels from DataFrame\n",
    "labels = sorted(ensemble.select(\"label\").distinct().rdd.map(lambda r: r[0]).collect())\n",
    "\n",
    "# Print per‚Äêclass Precision / Recall / F1\n",
    "print(\"Class‚Äêwise metrics:\")\n",
    "for lbl in labels:\n",
    "    print(f\" Class {int(lbl)} ‚Üí \"\n",
    "          f\"Precision: {metrics.precision(lbl):.4f},  \"\n",
    "          f\"Recall: {metrics.recall(lbl):.4f},  \"\n",
    "          f\"F1: {metrics.fMeasure(lbl):.4f}\")\n",
    "\n",
    "# 7) Overall metrics\n",
    "print(f\"\\nOverall Accuracy = {metrics.accuracy:.4f}\")\n",
    "print(f\"Weighted  F1      = {metrics.weightedFMeasure():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " w=0.0 ‚Üí F1=0.5527\n",
      " w=0.1 ‚Üí F1=0.5713\n",
      " w=0.2 ‚Üí F1=0.5872\n",
      " w=0.3 ‚Üí F1=0.5995\n",
      " w=0.4 ‚Üí F1=0.6075\n",
      " w=0.5 ‚Üí F1=0.6163\n",
      " w=0.6 ‚Üí F1=0.6213\n",
      " w=0.7 ‚Üí F1=0.6221\n",
      " w=0.8 ‚Üí F1=0.6170\n",
      " w=0.9 ‚Üí F1=0.6119\n",
      " w=1.0 ‚Üí F1=0.6110\n",
      "\n",
      "üîç Best weight on VAL ‚Üí w_LR=0.7, w_RF=0.3, F1=0.6221\n",
      "\n",
      "Test ‚Üí Accuracy: 0.6269, F1: 0.6229\n",
      "\n",
      "Confusion Matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|  206|\n",
      "|  0.0|       1.0|   50|\n",
      "|  0.0|       2.0|  146|\n",
      "|  1.0|       0.0|   36|\n",
      "|  1.0|       1.0|  168|\n",
      "|  1.0|       2.0|   95|\n",
      "|  2.0|       0.0|   97|\n",
      "|  2.0|       1.0|   61|\n",
      "|  2.0|       2.0|  441|\n",
      "+-----+----------+-----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types       import DoubleType\n",
    "from pyspark.ml              import PipelineModel\n",
    "from pyspark.ml.linalg       import Vectors, VectorUDT\n",
    "from pyspark.ml.evaluation   import MulticlassClassificationEvaluator\n",
    "\n",
    "import random, numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load tuned pipelines from S3\n",
    "lr_model = PipelineModel.load(\"s3://truegraph/models/bias_classifier_pca_tuned\")\n",
    "rf_model = PipelineModel.load(\"s3://truegraph/models/bias_rf_pca100_tuned\")\n",
    "\n",
    "# Read & cache all splits\n",
    "sdf = (\n",
    "    spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\")\n",
    "         .select(\"ID\",\n",
    "                 col(\"bias\").alias(\"label\").cast(\"double\"),\n",
    "                 col(\"content\").alias(\"text\"),\n",
    "                 \"split\")\n",
    "         .filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "         .cache()\n",
    ")\n",
    "sdf.count()\n",
    "\n",
    "# Split out validation & test\n",
    "val_df  = sdf.filter(col(\"split\")==\"valid\").cache();  val_df.count()\n",
    "test_df = sdf.filter(col(\"split\")==\"test\").cache();   test_df.count()\n",
    "\n",
    "# Pre-score both models on VALIDATION\n",
    "lr_val = (lr_model.transform(val_df)\n",
    "              .select(\"ID\",\"label\",\"probability\")\n",
    "              .withColumnRenamed(\"probability\",\"prob_lr\")\n",
    "              .cache())\n",
    "rf_val = (rf_model.transform(val_df)\n",
    "              .select(\"ID\",\"probability\")\n",
    "              .withColumnRenamed(\"probability\",\"prob_rf\")\n",
    "              .cache())\n",
    "lr_val.count(); rf_val.count()\n",
    "\n",
    "val_ens = lr_val.join(rf_val, on=\"ID\").cache()\n",
    "val_ens.count()\n",
    "\n",
    "# Build TEST ensemble likewise\n",
    "lr_test = (lr_model.transform(test_df)\n",
    "                .select(\"ID\",\"label\",\"probability\")\n",
    "                .withColumnRenamed(\"probability\",\"prob_lr\")\n",
    "                .cache())\n",
    "rf_test = (rf_model.transform(test_df)\n",
    "                .select(\"ID\",\"probability\")\n",
    "                .withColumnRenamed(\"probability\",\"prob_rf\")\n",
    "                .cache())\n",
    "lr_test.count(); rf_test.count()\n",
    "\n",
    "test_ens = lr_test.join(rf_test, on=\"ID\").cache()\n",
    "test_ens.count()\n",
    "\n",
    "# UDF for argmax\n",
    "argmax_udf = udf(lambda v: float(max(range(len(v)), key=lambda i: v[i])), DoubleType())\n",
    "\n",
    "# Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                                               predictionCol=\"prediction\",\n",
    "                                               metricName=\"f1\")\n",
    "\n",
    "# Grid-search weights on VAL\n",
    "best_w, best_f1 = None, -1.0\n",
    "for w in [i*0.1 for i in range(11)]:\n",
    "    weighted_udf = udf(\n",
    "        lambda v1, v2: Vectors.dense([w*x + (1-w)*y for x,y in zip(v1,v2)]),\n",
    "        VectorUDT()\n",
    "    )\n",
    "    preds = (val_ens\n",
    "        .withColumn(\"weighted_prob\", weighted_udf(\"prob_lr\",\"prob_rf\"))\n",
    "        .withColumn(\"prediction\",   argmax_udf(\"weighted_prob\"))\n",
    "    )\n",
    "    f1 = evaluator.evaluate(preds)\n",
    "    print(f\" w={w:.1f} ‚Üí F1={f1:.4f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_w = f1, w\n",
    "\n",
    "print(f\"\\nüîç Best weight on VAL ‚Üí w_LR={best_w:.1f}, w_RF={1-best_w:.1f}, F1={best_f1:.4f}\\n\")\n",
    "\n",
    "# Apply best weight to TEST\n",
    "weighted_udf = udf(\n",
    "    lambda v1, v2: Vectors.dense([best_w*x + (1-best_w)*y for x,y in zip(v1,v2)]),\n",
    "    VectorUDT()\n",
    ")\n",
    "test_preds = (test_ens\n",
    "    .withColumn(\"weighted_prob\", weighted_udf(\"prob_lr\",\"prob_rf\"))\n",
    "    .withColumn(\"prediction\",   argmax_udf(\"weighted_prob\"))\n",
    "    .withColumn(\"label\",        col(\"label\").cast(\"double\"))\n",
    "    .withColumn(\"prediction\",   col(\"prediction\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "# Final metrics\n",
    "acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\\\n",
    "      .evaluate(test_preds)\n",
    "f1  = evaluator.evaluate(test_preds)\n",
    "print(f\"Test ‚Üí Accuracy: {acc:.4f}, F1: {f1:.4f}\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "test_preds.groupBy(\"label\",\"prediction\").count()\\\n",
    "          .orderBy(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1748314322528_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-28-113.ec2.internal:20888/proxy/application_1748314322528_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-30-220.ec2.internal:8042/node/containerlogs/container_1748314322528_0004_01_000001/livy\">Link</a></td><td>‚úî</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Ensemble ‚Üí Test Accuracy: 0.6354, F1: 0.6339"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load your tuned base models\n",
    "lr_model = PipelineModel.load(\"s3://truegraph/models/bias_classifier_pca_tuned\")\n",
    "rf_model = PipelineModel.load(\"s3://truegraph/models/bias_rf_pca100_tuned\")\n",
    "\n",
    "# Load validation split \n",
    "sdf = spark.read.parquet(\"s3://truegraph/data/parquet/data_random_split\") \\\n",
    "    .select(\"ID\",\n",
    "            col(\"bias\").alias(\"label\").cast(\"double\"),\n",
    "            col(\"content\").alias(\"text\"),\n",
    "            \"split\") \\\n",
    "    .filter(col(\"label\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "val_df  = sdf.filter(col(\"split\") == \"valid\").cache()\n",
    "test_df = sdf.filter(col(\"split\") == \"test\").cache()\n",
    "\n",
    "# Generate base‚Äêmodel probabilities on validation\n",
    "lr_val = lr_model.transform(val_df) \\\n",
    "    .select(\"ID\",\"label\",\"probability\") \\\n",
    "    .withColumnRenamed(\"probability\",\"prob_lr\")\n",
    "\n",
    "rf_val = rf_model.transform(val_df) \\\n",
    "    .select(\"ID\",\"probability\") \\\n",
    "    .withColumnRenamed(\"probability\",\"prob_rf\")\n",
    "\n",
    "meta_train = lr_val.join(rf_val, on=\"ID\")\n",
    "\n",
    "# Assemble those two prob‚Äêvectors into one ‚Äúmeta‚Äù feature\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"prob_lr\",\"prob_rf\"],\n",
    "    outputCol=\"metaFeatures\"\n",
    ")\n",
    "meta_train = assembler.transform(meta_train).select(\"metaFeatures\",\"label\")\n",
    "\n",
    "# Train a small LogisticRegression as the meta‚Äêlearner\n",
    "meta_lr = LogisticRegression(\n",
    "    featuresCol=\"metaFeatures\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    regParam=0.1\n",
    ")\n",
    "meta_model = meta_lr.fit(meta_train)\n",
    "\n",
    "# Now build your meta‚Äêtest set from the actual test split\n",
    "lr_test = lr_model.transform(test_df) \\\n",
    "    .select(\"ID\",\"label\",\"probability\") \\\n",
    "    .withColumnRenamed(\"probability\",\"prob_lr\")\n",
    "\n",
    "rf_test = rf_model.transform(test_df) \\\n",
    "    .select(\"ID\",\"probability\") \\\n",
    "    .withColumnRenamed(\"probability\",\"prob_rf\")\n",
    "\n",
    "meta_test = lr_test.join(rf_test, on=\"ID\")\n",
    "meta_test = assembler.transform(meta_test).select(\"metaFeatures\",\"label\")\n",
    "\n",
    "# Apply the meta‚Äêmodel to get final predictions\n",
    "final = meta_model.transform(meta_test)\n",
    "\n",
    "# Evaluate\n",
    "e_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "e_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "print(f\"Stacked Ensemble ‚Üí Test Accuracy: {e_acc.evaluate(final):.4f}, F1: {e_f1.evaluate(final):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|  206|\n",
      "|  0.0|       1.0|   38|\n",
      "|  0.0|       2.0|  158|\n",
      "|  1.0|       0.0|   32|\n",
      "|  1.0|       1.0|  158|\n",
      "|  1.0|       2.0|  109|\n",
      "|  2.0|       0.0|   89|\n",
      "|  2.0|       1.0|   45|\n",
      "|  2.0|       2.0|  465|\n",
      "+-----+----------+-----+\n",
      "\n",
      "üìä Class-wise Precision / Recall / F1:\n",
      "+-----+------------------+------------------+------------------+\n",
      "|label|         precision|            recall|                f1|\n",
      "+-----+------------------+------------------+------------------+\n",
      "|    0|0.6299694189602446|0.5124378109452736|0.5651577503429356|\n",
      "|    1|0.6556016597510373|0.5284280936454849|0.5851851851851851|\n",
      "|    2|0.6352459016393442|0.7762938230383973|0.6987227648384672|\n",
      "+-----+------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "Overall Accuracy = 0.6377\n",
      "Weighted   F1 = 0.6313"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Load saved ensemble predictions\n",
    "ensemble = spark.read.parquet(\"s3://truegraph/models/ensemble_preds_test\") \\\n",
    "               .select(\"label\", \"prediction\")\n",
    "\n",
    "# Build & show the confusion matrix\n",
    "conf_df = (ensemble\n",
    "  .groupBy(\"label\",\"prediction\")\n",
    "  .count()\n",
    "  .orderBy(\"label\",\"prediction\")\n",
    ")\n",
    "conf_df.show()\n",
    "\n",
    "# Compute metrics via MulticlassMetrics\n",
    "rdd = ensemble.rdd.map(lambda r: (float(r.prediction), float(r.label)))\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "\n",
    "# Get the distinct labels from the DataFrame\n",
    "labels = sorted([row[0] for row in ensemble.select(\"label\").distinct().collect()])\n",
    "\n",
    "# Build a Spark DataFrame of per-class metrics\n",
    "rows = []\n",
    "for lbl in labels:\n",
    "    rows.append(( \n",
    "        int(lbl),\n",
    "        float(metrics.precision(lbl)),\n",
    "        float(metrics.recall(lbl)),\n",
    "        float(metrics.fMeasure(lbl))\n",
    "    ))\n",
    "prf_df = spark.createDataFrame(rows, schema=[\"label\",\"precision\",\"recall\",\"f1\"])\n",
    "\n",
    "# Display it\n",
    "print(\" Class-wise Precision / Recall / F1:\")\n",
    "prf_df.show()\n",
    "\n",
    "# Also print overall stats\n",
    "print(f\"\\nOverall Accuracy = {metrics.accuracy:.4f}\")\n",
    "print(f\"Weighted   F1 = {metrics.weightedFMeasure():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
